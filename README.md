ğŸ  House Price Prediction using Machine Learning
ğŸ“Œ Project Overview

This project focuses on building an end-to-end machine learning pipeline to predict house prices using structured real-estate data. The goal was not just to train a model, but to follow a proper data science workflow including EDA, feature engineering, regularization, and model evaluation.

The final solution was evaluated on the Kaggle House Prices competition, achieving a strong score.

ğŸ¯ Problem Statement

Predict the SalePrice of houses based on various numerical and categorical features such as location, size, condition, and amenities.

This is a regression problem with a continuous target variable.

ğŸ“‚ Dataset

Source: Kaggle â€“ House Prices: Advanced Regression Techniques

Files used:

train.csv â€“ Training data with target variable

test.csv â€“ Test data without target variable

ğŸ§  Approach & Methodology
1ï¸âƒ£ Exploratory Data Analysis (EDA)

Analyzed feature types (numerical vs categorical)

Identified missing values and their distribution

Inspected target variable (SalePrice) and observed right-skewness

Studied the impact of missing values on the target variable

2ï¸âƒ£ Missing Value Handling

Numerical features filled using median

Categorical features filled using mode

Decision was data-driven based on EDA results

3ï¸âƒ£ Feature Engineering

Converted categorical variables using one-hot encoding

Ensured consistency between training and test datasets

Removed data inconsistencies introduced by encoding

4ï¸âƒ£ Feature Scaling

Applied standardization using StandardScaler

Required for regularized linear models such as Lasso, Ridge, and ElasticNet

5ï¸âƒ£ Feature Selection

Used Lasso-based regularization to reduce dimensionality

Removed weak and noisy features to prevent overfitting

6ï¸âƒ£ Target Transformation

Applied log transformation (log1p) on SalePrice

Reduced skewness and improved model generalization

7ï¸âƒ£ Modeling & Evaluation

Built clean and reproducible scikit-learn pipelines

Compared:

Ridge Regression

ElasticNet Regression

Evaluation Metric:

Root Mean Squared Error (RMSE) on log-transformed target

ğŸ† Results

Final Kaggle Score: 0.18394 (Log-RMSE)

Best Model: ElasticNet Regression

Demonstrated strong generalization with reduced overfitting

ğŸ› ï¸ Tech Stack

Python

Pandas, NumPy

Matplotlib, Seaborn

Scikit-learn

Jupyter Notebook

ğŸ“ˆ Key Learnings

Importance of EDA before modeling

Handling skewed targets using log transformation

Role of regularization in reducing overfitting

Advantages of pipelines for preventing data leakage

Proper Kaggle submission workflow

ğŸš€ Future Improvements

Hyperparameter tuning using cross-validation

Experiment with tree-based models (Random Forest, XGBoost, LightGBM)

Feature interaction engineering

Deployment as a web application

ğŸ“ Kaggle Submission

Competition: House Prices â€“ Advanced Regression Techniques

Public Score: 0.18394

ğŸ‘¤ Author

Darsh Jilka
